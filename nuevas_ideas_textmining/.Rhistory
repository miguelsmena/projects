ggplot(aes(x = has_image, fill = factor(spam)) +
# Create plot of proportion of spam by image
emailt %>%
mutate(has_image = image >0) %>%
ggplot(aes(x = has_image, fill = factor(spam))) +
geom_bar(position = 'fill')
emailt %>%
mutate(has_image = image >0) %>%
ggplot(aes(x = has_image, fill = factor(spam))) +
geom_bar(position = 'fill')
emailt %>%
mutate(has_image = image >0) %>%
ggplot(aes(x = has_image, fill = factor(spam))) +
geom_bar(position = 'fill')
# Reorder levels
emailt$number <- factor(email$number, levels = c('none', 'small', 'big'))
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam)
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam, labeller = "he")
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam, labeller = label_both())
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam, labeller = label_both(labels = c(1,2)))
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam, labeller = label_both(supp = C("1", "2")))
# Construct plot of number
ggplot(emailt, aes(x=number)) +
geom_bar() +
facet_wrap(~spam)
#Supervised ML
library(class)
library(tidyverse)
#read data
signs <- read_csv(file = "https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
signs
glimpse(signs)
View(signs)
View(signs)
#traing my first knn algorithm
#first: Create a vector of labels
sign_types <- signs$sign_type
next_sign <- read_csv(file='https://docs.google.com/spreadsheets/d/e/2PACX-1vTR5iw1X5vVLfWa70UM5NP_1V4SMLkkLegqUVDHeJ6mfyGbPv7r57CNo5NYfeNAJgELqXNSzJgO2QjQ/pub?output=csv')
View(signs)
View(signs)
View(next_sign)
knn(train = signs[-1], test = next_sign, cl = sign_types) #signs
next_sign <- read_csv(file='https://docs.google.com/spreadsheets/d/e/2PACX-1vTR5iw1X5vVLfWa70UM5NP_1V4SMLkkLegqUVDHeJ6mfyGbPv7r57CNo5NYfeNAJgELqXNSzJgO2QjQ/pub?output=csv')
knn(train = signs[-1], test = next_sign, cl = sign_types) #signs
sign[-1]
signs[-1]
trial <-signs[-1]
View(trial)
View(trial)
View(signs)
View(signs)
next_sign <- read_csv(file='https://docs.google.com/spreadsheets/d/e/2PACX-1vTR5iw1X5vVLfWa70UM5NP_1V4SMLkkLegqUVDHeJ6mfyGbPv7r57CNo5NYfeNAJgELqXNSzJgO2QjQ/pub?output=csv')
knn(train = trial, test = next_sign, cl = sign_types) #signs
typeof(trial)
typeof(next_sign)
View(next_sign)
next_sign <- read_csv(file='https://docs.google.com/spreadsheets/d/e/2PACX-1vTR5iw1X5vVLfWa70UM5NP_1V4SMLkkLegqUVDHeJ6mfyGbPv7r57CNo5NYfeNAJgELqXNSzJgO2QjQ/pub?output=csv')
#traing my first knn algorithm
#first: Create a vector of labels
sign_types <- signs$sign_type
trial <-signs[-1]
knn(train = trial, test = next_sign, cl = sign_types) #signs
View(signs)
View(signs)
#read data
signs <- read_csv(file = "https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
next_sign <- read_csv(file='https://docs.google.com/spreadsheets/d/e/2PACX-1vTR5iw1X5vVLfWa70UM5NP_1V4SMLkkLegqUVDHeJ6mfyGbPv7r57CNo5NYfeNAJgELqXNSzJgO2QjQ/pub?output=csv')
#traing my first knn algorithm
#first: Create a vector of labels
sign_types <- signs$sign_type
trial <-signs[-1]
knn(train = trial, test = next_sign, cl = sign_types) #signs
trial <-signs[-2]
View(signs)
View(signs)
trial <-signs[-2]
View(trial)
View(trial)
trial <-signs[-1]
View(trial)
View(trial)
trial <-signs[-2]
trial <-signs[-1]
trial <-signs[-2]
trial <-signs[-2]
trial <-signs[-1]
trial <-signs[-2]
View(trial)
str(trial)
trial <-signs[2:51]
str(trial)
trial <-signs[3:51]
str(trial)
next_sign <- signs[,-1]
#EDA on comics dataset (categorical data)
library(tidyverse)
library(forcats)
#read comics data and drop any level that is zero
comics <- read_csv(file = 'https://assets.datacamp.com/production/repositories/537/datasets/8860af2c0ef67fc77a8c704a73bbb93a395debcf/comics.csv', col_types = cols(
name = col_character(),
id = col_character(),
align = col_factor(),
eye = col_character(),
hair = col_character(),
gender = col_factor(),
gsm = col_character(),
alive = col_character(),
appearances = col_double(),
first_appear = col_character(),
publisher = col_character()
)) %>% filter(align != 'Reformed Criminals') %>%
droplevels()
#read some factors
align_f <- levels(comics$align)
gender_f <- levels(comics$gender)
#Perform some contingency tables
table_1 <- table(comics$align, comics$gender)
#Perform some proportion table
table_prop <- prop.table(table_1, 2)
#perform a stacked graphic
graph_1 <- ggplot(comics, aes(x = align, fill = gender)) +
geom_bar(position = "fill")
graph_1
#EDA on cars dataset (continous data)
cars <- read_csv(file = 'https://assets.datacamp.com/production/repositories/537/datasets/c0366d5da5ee8dce49919a5443685cf2e50c6a96/cars04.csv', col_types = cols(
name = col_character(),
sports_car = col_logical(),
suv = col_logical(),
wagon = col_logical(),
minivan = col_logical(),
pickup = col_logical(),
all_wheel = col_logical(),
rear_wheel = col_logical(),
msrp = col_double(),
dealer_cost = col_double(),
eng_size = col_double(),
ncyl = col_double(),
horsepwr = col_double(),
city_mpg = col_double(),
hwy_mpg = col_double(),
weight = col_double(),
wheel_base = col_double(),
length = col_double(),
width = col_double()
))
#histogram to check the distribution of continous data
cars_hist <- ggplot(cars, aes(x= city_mpg)) +
geom_histogram() +
facet_grid(.~suv)
cars_hist
#box plot to check the distribution
#first filter the most common ncyl
unique(cars$ncyl)
common_cyl <- cars %>%
filter(ncyl %in% c(4,6,8))
#create the boxplot, so good to detect outliers
cars_box <- ggplot(common_cyl, aes(x= as.factor(ncyl), y= city_mpg)) +
geom_boxplot()
cars_box
library(tidyverse)
data <- read_csv('https://assets.datacamp.com/production/repositories/718/datasets/571628c39048df59c40c9dcfba146a2cf7a4a0e3/locations.csv')
p_A <- data %>% count(office)
install.packages('tidytuesdayR')
version()
version
library(tidyverse)
cran_code <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-12/loc_cran_packages.csv")
cran_code
cran_code %>%
ggplot(aes(language)) +
geom_bar()
unique(cran_code$language)
cran_code %>%
top_n(language, 10)
cran_code %>%
top_n(cran_code$language, 10)
cran_code %>%
count(cran_code$language)
cran_code %>%
count(cran_code$language) %>%
arrange()
cran_code %>%
count(cran_code$language) %>%
arrange(n)
cran_code %>%
count(cran_code$language) %>%
arrange(desc(n))
cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(language)) +
geom_bar()
cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(cran_code$language)) +
geom_bar()
cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(x=language, y=n)) +
geom_bar()
cran_code_top_language <- cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(x=language, y=n)) +
geom_bar()
cran_code_top_language
cran_code_top_language <- cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(xcran_code=language, y=n)) +
geom_bar()
cran_code_top_language <- cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(xcran_code$language, y=n)) +
geom_bar()
cran_code_top_language
cran_code_top_language <- cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10) %>%
ggplot(aes(cran_code$language, y=n)) +
geom_bar()
cran_code_top_language
cran_code_top_language <- cran_code %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
cran_code_top_language
cran_code_top_language %>%
ggplot(aes(`cran_code$language`, n)) +
geom_bar()
cran_code_top_language %>%
ggplot(aes(`cran_code$language`, n)) +
geom_col()
cran_code_top_language <- cran_code %>%
filter(language != 'R')
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
cran_code_top_language <- cran_code %>%
filter(language !'R')
cran_code_top_language <- cran_code %>%
filter(language != "R")
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
#-------------
cran_code_1 %>%
cran_code %>%
filter(language != 'R')
#-------------
cran_code_1 <- cran_code %>%
filter(language != 'R')
cran_code_1
unique(cran_code_1$language)
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
count(cran_code$language) %>%
arrange(desc(n)) %>%
top_n(10)
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
count(cran_code$language)
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
group_by(language) %>%
summarise(count=n())
cran_code_top_language
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
group_by(language) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
top_n(count)
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
group_by(language) %>%
summarise(count=n()) %>%
arrange(desc(count))
cran_code_top_language
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
group_by(language) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
top_n(10)
cran_code_top_language
cran_code_top_language %>%
ggplot(aes(language, count)) +
geom_col()
library(tidyverse)
cran_code <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-12/loc_cran_packages.csv")
cran_code_top_language <- cran_code %>%
filter(language != "R") %>%
group_by(language) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
top_n(10)
discrete <- c('hello', 'hello', 'bye', 'bye')
discrete$count = n()
discrete$count = n(discrete)
---
title: "ppto_2020_tweet"
output: html_document
author: Miguel Sanchez
---
``` {r}
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```
``` {r, echo=FALSE}
``` {r echo=FALSE}
---
title: "ppto_2020_tweet"
output: html_document
author: Miguel Sanchez
---
``` {r echo=FALSE}
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```
---
title: "ppto_2020_tweet"
output: html_document
author: Miguel Sanchez
---
``` {r echo=FALSE}
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```
``` {r}
---
title: "ppto_2020_tweet"
output: html_document
author: Miguel Sanchez
---
```{r}
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#set libraries
library(rtweet)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
#set working eviroment
setwd('C:/Users/migue/rfiles/nuevas_ideas_textmining')
#twitter keys
appname <- 'mikesanchezapp'
key <- "CdqGMRtGg0PBlXsPtyflnEAu1"
secret_key <- "xfzrbFeWIgpJGwkheOyJhSqOGT8hrxVLlKe0wT0GxmflKBWm4V"
#twitter tokens
access_token <- "788076739-cmreHoPN9QfPcWC9DqGvTj9TkX8n00XVt8yXKnlX"
access_secret <- "dLBzATb27T6fsG3lzaFhEzXFQ8ckHAR2xLCbx64aLpy2w"
#create eviroment
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret_key,
access_token = access_token,
access_secret = access_secret)
twitter_token
#twitter keys
appname <- 'mikesanchezapp'
key <- "CdqGMRtGg0PBlXsPtyflnEAu1"
secret_key <- "xfzrbFeWIgpJGwkheOyJhSqOGT8hrxVLlKe0wT0GxmflKBWm4V"
#twitter tokens
access_token <- "788076739-cmreHoPN9QfPcWC9DqGvTj9TkX8n00XVt8yXKnlX"
access_secret <- "dLBzATb27T6fsG3lzaFhEzXFQ8ckHAR2xLCbx64aLpy2w"
#create eviroment
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret_key,
access_token = access_token,
access_secret = access_secret)
twitter_token
#read twitter data
ppto_2020 <- search_tweets( q = '#PRESUPUESTO2020YA',
n = 500,
include_rts = FALSE)
ppto_2020t_txt <- ppto_2020 %>%
select(text)
write.csv(ppto_2020_tidy, "ppto_2020_tidy_flat.csv")
ppto_2020t_txt <- ppto_2020 %>%
select(text)
write.csv(ppto_2020_txt, "ppto_2020_tidy_flat.csv")
ppto_2020_txt <- ppto_2020 %>%
select(text)
write.csv(ppto_2020_txt, "ppto_2020_tidy_flat.csv")
#inspect data
head(ppto_2020_txt)
#tidy data
#csv collected 12/12/19 with 6-9 days of tweets
ppto_2020_flat <- read_csv("ppto_2020_tidy_flat.csv") %>%
select(text) %>%
unnest_tokens(word, text) %>%
filter(word != "PRESUPUESTO2020YA")
#tidy data
#csv collected 12/12/19 with 6-9 days of tweets
ppto_2020_flat <- read_csv("ppto_2020_tidy_flat.csv") %>%
select(text) %>%
unnest_tokens(word, text) %>%
filter(word != "PRESUPUESTO2020YA")
#tidy data
#csv collected 12/12/19 with 6-9 days of tweets
ppto_2020_flat <- read_csv("ppto_2020_tidy_flat.csv") %>%
select(text) %>%
unnest_tokens(word, text) %>%
filter(word != "PRESUPUESTO2020YA")
#ncr
ncr_select_string <- c('Spanish (es)', 'Positive', 'Negative', 'Anger',	'Anticipation',	'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust')
ncr_spanish <- readxl:: read_xlsx('NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx') %>%
select(ncr_select_string) %>%
rename(word = `Spanish (es)`) %>%
gather(-word, key = 'sentiment', value = 'value') %>%
filter(value == 1, word != 'NO TRANSLATION') %>%
select(-value)
#ncr
ncr_select_string <- c('Spanish (es)', 'Positive', 'Negative', 'Anger',	'Anticipation',	'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust')
ncr_spanish <- readxl:: read_xlsx('NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx') %>%
select(ncr_select_string) %>%
rename(word = `Spanish (es)`) %>%
gather(-word, key = 'sentiment', value = 'value') %>%
filter(value == 1, word != 'NO TRANSLATION') %>%
select(-value)
#inner join
ppto_join <- ppto_2020_flat %>%
inner_join(ncr_spanish, by='word') %>%
count(word)
#inner join
ppto_join <- ppto_2020_flat %>%
inner_join(ncr_spanish, by='word') %>%
count(word)
set.seed(1234)
ppto_2020_cloud <- wordcloud(words = ppto_join$word,
freq = ppto_join$n,
min.freq = 1,
max.words= 500,
random.order = FALSE,
random.color = FALSE,
colors=brewer.pal(8, "Dark2"))
